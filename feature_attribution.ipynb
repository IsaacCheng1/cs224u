{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [InputXGradients](#InputXGradients)\n",
    "1. [Selectivity examples](#Selectivity-examples)\n",
    "1. [Simple feed-forward classifier example](#Simple-feed-forward-classifier-example)\n",
    "1. [Bag-of-words classifier for the SST](#Bag-of-words-classifier-for-the-SST)\n",
    "1. [BERT example](#BERT-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is an experimental extension of the CS224u course code. It focuses on the [Integrated Gradients](https://arxiv.org/abs/1703.01365) method for feature attribution, with comparisons to the \"inputs $\\times$ gradients\" method. To run the notebook, first install [the Captum library](https://captum.ai/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: captum in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from captum) (3.4.3)\n",
      "Requirement already satisfied: torch>=1.6 in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from captum) (1.10.0)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from captum) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from torch>=1.6->captum) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from matplotlib->captum) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from matplotlib->captum) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from matplotlib->captum) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from matplotlib->captum) (1.3.1)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/envs/nlu/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->captum) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not currently a required installation (but it will be in future years)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InputXGradients\n",
    "\n",
    "For both implementations, the `forward` method of `model` is used. `X` is an (m x n) tensor of attributions. Use `targets=None` for models with scalar outputs, else supply a LongTensor giving a label for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def grad_x_input(model, X, targets=None):\n",
    "    \"\"\"Implementation using PyTorch directly.\"\"\"\n",
    "    X.requires_grad = True\n",
    "    y = model(X)\n",
    "    y = y if targets is None else y[list(range(len(y))), targets]\n",
    "    (grads, ) = torch.autograd.grad(y.unbind(), X)\n",
    "    return grads * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import InputXGradient\n",
    "\n",
    "def captum_grad_x_input(model, X, target):\n",
    "    \"\"\"Captum-based implementation.\"\"\"\n",
    "    X.requires_grad = True\n",
    "    amod = InputXGradient(model)\n",
    "    return amod.attribute(X, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selectivity examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import InputXGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectivityAssessor(nn.Module):\n",
    "    \"\"\"Model used by Sundararajan et al, section 2.1 to show that\n",
    "    input * gradients violates their selectivity axiom.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return 1.0 - self.relu(1.0 - X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_mod = SelectivityAssessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple inputs with just one feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = torch.FloatTensor([[0.0], [2.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs for our two examples differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_mod(X_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `InputXGradient` assigns the same importance to the feature across the two examples, violating selectivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [-0.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captum_grad_x_input(sel_mod, X_sel, target=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated gradients addresses the problem by averaging gradients across all interpolated representations between the baseline and the actual input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_sel = IntegratedGradients(sel_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_baseline = torch.FloatTensor([[0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.]], dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_sel.attribute(X_sel, sel_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A toy implementation to help bring out what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ig_reference_implementation(model, x, base, m=50):\n",
    "    vals = []\n",
    "    for k in range(m):\n",
    "        # Interpolated representation:\n",
    "        xx = (base + (k/m)) * (x - base)\n",
    "        # Gradient for the interpolated example:\n",
    "        xx.requires_grad = True\n",
    "        y = model(xx)\n",
    "        (grads, ) = torch.autograd.grad(y.unbind(), xx)\n",
    "        vals.append(grads)\n",
    "    return (1 / m) * torch.cat(vals).sum(axis=0) * (x - base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_reference_implementation(sel_mod, torch.FloatTensor([[20.0]]), sel_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward classifier example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cls, y_cls = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_classes=3,\n",
    "    n_features=5,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification problem has two uninformative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20138107, 0.02833358, 0.11584416, 0.        , 0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_classif(X_cls, y_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(X_cls, y_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TorchShallowNeuralClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 399. Training loss did not improve more than tol=1e-05. Final error is 1.4130553603172302."
     ]
    }
   ],
   "source": [
    "_ = classifier.fit(X_cls_train, y_cls_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_preds = classifier.predict(X_cls_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8648"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_cls_test, cls_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_ig = IntegratedGradients(classifier.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_baseline = torch.zeros(1, X_cls_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated gradients with respect to the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_attrs = classifier_ig.attribute(\n",
    "    torch.FloatTensor(X_cls_test),\n",
    "    classifier_baseline,\n",
    "    target=torch.LongTensor(y_cls_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average attribution is low for the two uninformative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0526,  0.6108,  0.4918, -0.0254, -0.0277], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_attrs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words classifier for the SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from captum.attr import IntegratedGradients\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join(\"data\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-word featurization with stopword removal to make this a little easier to study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xianbing/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def phi(text):\n",
    "    return Counter([w for w in text.lower().split() if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mlp(X, y):\n",
    "    mod = TorchShallowNeuralClassifier(early_stopping=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 41. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.5985297821462154"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.633     0.673     0.652       428\n",
      "     neutral      0.293     0.157     0.205       229\n",
      "    positive      0.639     0.752     0.691       444\n",
      "\n",
      "    accuracy                          0.598      1101\n",
      "   macro avg      0.521     0.527     0.516      1101\n",
      "weighted avg      0.564     0.598     0.575      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = sst.experiment(\n",
    "    sst.train_reader(SST_HOME),\n",
    "    phi,\n",
    "    fit_mlp,\n",
    "    sst.dev_reader(SST_HOME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_classifier = experiment['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captum needs to have labels as indices rather than strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_classifier.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sst_test = [sst_classifier.classes_.index(label)\n",
    "              for label in experiment['assess_datasets'][0]['y']]\n",
    "\n",
    "sst_preds = [sst_classifier.classes_.index(label)\n",
    "             for label in experiment['predictions'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our featurized test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sst_test = experiment['assess_datasets'][0]['X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature names to help with analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xianbing/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "fnames = experiment['train_dataset']['vectorizer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '!?',\n",
       " '#',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'30s\",\n",
       " \"'40s\",\n",
       " \"'50s\",\n",
       " \"'53\",\n",
       " \"'60s\",\n",
       " \"'70s\",\n",
       " \"'80s\",\n",
       " \"'90s\",\n",
       " \"'d\",\n",
       " \"'em\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'n\",\n",
       " \"'n'\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'til\",\n",
       " \"'ve\",\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '-lrb-',\n",
       " '-rrb-',\n",
       " '.',\n",
       " '...',\n",
       " '1',\n",
       " '1.2',\n",
       " '1.8',\n",
       " '10',\n",
       " '10,000',\n",
       " '10-course',\n",
       " '10-year',\n",
       " '10-year-old',\n",
       " '100',\n",
       " '100-minute',\n",
       " '100-year',\n",
       " '101',\n",
       " '102-minute',\n",
       " '103-minute',\n",
       " '104',\n",
       " '105',\n",
       " '10th',\n",
       " '10th-grade',\n",
       " '11',\n",
       " '110',\n",
       " '112-minute',\n",
       " '12',\n",
       " '12-year-old',\n",
       " '120',\n",
       " '127',\n",
       " '129-minute',\n",
       " '12th',\n",
       " '13',\n",
       " '13th',\n",
       " '14-year-old',\n",
       " '140',\n",
       " '146',\n",
       " '15',\n",
       " '15-year',\n",
       " '15th',\n",
       " '163',\n",
       " '168-minute',\n",
       " '170',\n",
       " '1790',\n",
       " '18',\n",
       " '18-year-old',\n",
       " '1899',\n",
       " '19',\n",
       " '1915',\n",
       " '1920',\n",
       " '1930s',\n",
       " '1933',\n",
       " '1937',\n",
       " '1938',\n",
       " '1940s',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1952',\n",
       " '1953',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1962',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1975',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1984',\n",
       " '1986',\n",
       " '1987',\n",
       " '1989',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '19th-century',\n",
       " '1\\\\/2',\n",
       " '2',\n",
       " '2,500',\n",
       " '2-day',\n",
       " '20',\n",
       " '20,000',\n",
       " '20-car',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '20th',\n",
       " '20th-century',\n",
       " '21\\\\/2',\n",
       " '21st',\n",
       " '22',\n",
       " '22-year-old',\n",
       " '24-and-unders',\n",
       " '2455',\n",
       " '24\\\\/7',\n",
       " '25',\n",
       " '26',\n",
       " '26-year-old',\n",
       " '270',\n",
       " '295',\n",
       " '2\\\\/3',\n",
       " '3',\n",
       " '3-d',\n",
       " '3-year-olds',\n",
       " '30',\n",
       " '30-year',\n",
       " '300',\n",
       " '3000',\n",
       " '37-minute',\n",
       " '3\\\\/4th',\n",
       " '3d',\n",
       " '4',\n",
       " '40',\n",
       " '42',\n",
       " '451',\n",
       " '48',\n",
       " '4\\\\/5ths',\n",
       " '4ever',\n",
       " '4w',\n",
       " '5',\n",
       " '50',\n",
       " '50-million',\n",
       " '50-something',\n",
       " '50-year',\n",
       " '50-year-old',\n",
       " '50s',\n",
       " '51',\n",
       " '51st',\n",
       " '52',\n",
       " '6',\n",
       " '6-year-old',\n",
       " '60',\n",
       " '60-second',\n",
       " '65',\n",
       " '65-minute',\n",
       " '65-year-old',\n",
       " '65th',\n",
       " '66',\n",
       " '7',\n",
       " '70-year-old',\n",
       " '70s',\n",
       " '71',\n",
       " '72',\n",
       " '72-year-old',\n",
       " '75',\n",
       " '75-minute',\n",
       " '77',\n",
       " '78',\n",
       " '7th',\n",
       " '7th-century',\n",
       " '8',\n",
       " '8-10',\n",
       " '8-year-old',\n",
       " '80',\n",
       " '80-minute',\n",
       " '800',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '85-minute',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '88-minute',\n",
       " '89',\n",
       " '8th',\n",
       " '9',\n",
       " '9-11',\n",
       " '90',\n",
       " '90-minute',\n",
       " '90-plus',\n",
       " '91-minute',\n",
       " '93',\n",
       " '94',\n",
       " '94-minute',\n",
       " '95',\n",
       " '95-minute',\n",
       " '96',\n",
       " '98',\n",
       " '99',\n",
       " '99-minute',\n",
       " '9\\\\/11',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '?!?',\n",
       " '\\\\*',\n",
       " '\\\\*\\\\*',\n",
       " '\\\\*\\\\*\\\\*',\n",
       " '\\\\*\\\\*\\\\*\\\\*',\n",
       " '\\\\/',\n",
       " '`',\n",
       " '``',\n",
       " 'a-bornin',\n",
       " 'a-knocking',\n",
       " 'a-list',\n",
       " 'a-team',\n",
       " 'a.',\n",
       " 'a.c.',\n",
       " 'a.e.w.',\n",
       " 'a.s.',\n",
       " 'aaa',\n",
       " 'aaliyah',\n",
       " 'aan',\n",
       " 'abagnale',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abbass',\n",
       " 'abbott',\n",
       " 'abbreviated',\n",
       " 'abc',\n",
       " 'abderrahmane',\n",
       " 'abdul',\n",
       " 'abel',\n",
       " 'abhorrent',\n",
       " 'abhors',\n",
       " 'abiding',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'able',\n",
       " 'ably',\n",
       " 'abomination',\n",
       " 'aboriginal',\n",
       " 'aborted',\n",
       " 'abound',\n",
       " 'above-average',\n",
       " 'abrasive',\n",
       " 'abridged',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdist',\n",
       " 'absurdities',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accent',\n",
       " 'accents',\n",
       " 'accentuating',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accident-prone',\n",
       " 'accidental',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'accommodate',\n",
       " 'accomodates',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulates',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'ace',\n",
       " 'acerbic',\n",
       " 'ache',\n",
       " 'achero',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'aching',\n",
       " 'achingly',\n",
       " 'achival',\n",
       " 'achronological',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acidity',\n",
       " 'ackerman',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acolytes',\n",
       " 'acquainted',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquires',\n",
       " 'acres',\n",
       " 'acrid',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'acting-workshop',\n",
       " 'action',\n",
       " 'action-adventure',\n",
       " 'action-and-popcorn',\n",
       " 'action-comedy',\n",
       " 'action-movie',\n",
       " 'action-oriented',\n",
       " 'action-packed',\n",
       " 'action-thriller\\\\/dark',\n",
       " 'action\\\\/comedy',\n",
       " 'action\\\\/thriller',\n",
       " 'actioner',\n",
       " 'actioners',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activism',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actor\\\\/director',\n",
       " 'actorish',\n",
       " 'actorliness',\n",
       " 'actorly',\n",
       " 'actors',\n",
       " 'actory',\n",
       " 'actress',\n",
       " 'actress-producer',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actuary',\n",
       " 'acumen',\n",
       " 'acute',\n",
       " 'ad',\n",
       " 'adage',\n",
       " 'adam',\n",
       " 'adamant',\n",
       " 'adams',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapts',\n",
       " 'add',\n",
       " 'addams',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'address',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adept',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhere',\n",
       " 'adherents',\n",
       " 'adhering',\n",
       " 'adjective',\n",
       " 'adjusting',\n",
       " 'administration',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admirer',\n",
       " 'admirers',\n",
       " 'admiring',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'ado',\n",
       " 'adobo',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolescents',\n",
       " 'adopt',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adorably',\n",
       " 'adored',\n",
       " 'adoring',\n",
       " 'adorns',\n",
       " 'adrenalin',\n",
       " 'adrenaline',\n",
       " 'adrenalized',\n",
       " 'adrian',\n",
       " 'adrien',\n",
       " 'adrift',\n",
       " 'adroit',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adultery',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'adventues',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'adversity',\n",
       " 'advert',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'advises',\n",
       " 'advocacy',\n",
       " 'aerial',\n",
       " 'aesop',\n",
       " 'aesthetic',\n",
       " 'aesthetically',\n",
       " 'aesthetics',\n",
       " 'affability',\n",
       " 'affable',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affectation',\n",
       " 'affectation-free',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affectingly',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affectionately',\n",
       " 'affections',\n",
       " 'affects',\n",
       " 'affinity',\n",
       " 'affirm',\n",
       " 'affirmational',\n",
       " 'affirming',\n",
       " 'affirms',\n",
       " 'affleck',\n",
       " 'afflicts',\n",
       " 'affluence',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affords',\n",
       " 'afghan',\n",
       " 'afghani',\n",
       " 'aficionados',\n",
       " 'afloat',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'african-american',\n",
       " 'african-americans',\n",
       " 'after-hours',\n",
       " 'after-school',\n",
       " 'afterlife',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afterschool',\n",
       " 'aftertaste',\n",
       " 'afterthought',\n",
       " 'afterwards',\n",
       " 'agape',\n",
       " 'age',\n",
       " 'age-inspired',\n",
       " 'age-wise',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agendas',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'ages-old',\n",
       " 'aggrandizing',\n",
       " 'aggravating',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aggressiveness',\n",
       " 'aggrieved',\n",
       " 'agile',\n",
       " 'aging',\n",
       " 'agitator',\n",
       " 'agitprop',\n",
       " 'agnostic',\n",
       " 'ago',\n",
       " 'agonizing',\n",
       " 'agony',\n",
       " 'agreeably',\n",
       " 'agreed',\n",
       " 'agreement',\n",
       " 'aground',\n",
       " 'ah',\n",
       " 'ah-nuld',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahola',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aided',\n",
       " 'aids',\n",
       " 'aiello',\n",
       " 'ailments',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " 'aimlessness',\n",
       " 'aims',\n",
       " 'air',\n",
       " 'air-conditioning',\n",
       " 'aircraft',\n",
       " 'airhead',\n",
       " 'airless',\n",
       " 'airs',\n",
       " 'airy',\n",
       " 'aisle',\n",
       " 'aisles',\n",
       " 'akin',\n",
       " 'al.',\n",
       " 'alabama',\n",
       " 'alacrity',\n",
       " 'aladdin',\n",
       " 'alagna',\n",
       " 'alain',\n",
       " 'alan',\n",
       " 'alarming',\n",
       " 'alarms',\n",
       " 'alas',\n",
       " 'albeit',\n",
       " 'album',\n",
       " 'alcatraz',\n",
       " 'alchemical',\n",
       " 'aldrich',\n",
       " 'alert',\n",
       " 'alexander',\n",
       " 'alexandre',\n",
       " 'alfonso',\n",
       " 'alfred',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'alienating',\n",
       " 'alienation',\n",
       " 'aliens',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'all-around',\n",
       " 'all-enveloping',\n",
       " 'all-french',\n",
       " 'all-inclusive',\n",
       " 'all-male',\n",
       " 'all-night',\n",
       " 'all-out',\n",
       " 'all-over-the-map',\n",
       " 'all-powerful',\n",
       " 'all-star',\n",
       " 'all-time',\n",
       " 'all-too-familiar',\n",
       " 'all-too-human',\n",
       " 'all-woman',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allegory',\n",
       " 'allen',\n",
       " 'allied',\n",
       " 'allison',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alluring',\n",
       " 'allusions',\n",
       " 'ally',\n",
       " 'almodovar',\n",
       " 'almost',\n",
       " 'aloft',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'aloof',\n",
       " 'already',\n",
       " 'already-shallow',\n",
       " 'also',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'alternate',\n",
       " 'alternately',\n",
       " 'alternates',\n",
       " 'alternating',\n",
       " 'alternative',\n",
       " 'alternatives',\n",
       " 'although',\n",
       " 'altman',\n",
       " 'altman-esque',\n",
       " 'altogether',\n",
       " 'always',\n",
       " 'amalgam',\n",
       " 'amari',\n",
       " 'amaro',\n",
       " 'amassed',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amateurishly',\n",
       " 'amaze',\n",
       " 'amazement',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'ambience',\n",
       " 'ambiguities',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'ambitiously',\n",
       " 'ambivalence',\n",
       " 'ambivalent',\n",
       " 'amble',\n",
       " 'ambrose',\n",
       " 'amc',\n",
       " 'ame',\n",
       " 'america',\n",
       " 'american',\n",
       " 'american-russian',\n",
       " 'american-style',\n",
       " 'americanized',\n",
       " 'americans',\n",
       " 'amiable',\n",
       " 'amiably',\n",
       " 'amicable',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amini',\n",
       " 'amir',\n",
       " 'amish',\n",
       " 'amiss',\n",
       " 'amnesiac',\n",
       " 'amok',\n",
       " 'among',\n",
       " 'amoral',\n",
       " 'amorality',\n",
       " 'amos',\n",
       " 'amount',\n",
       " 'amounts',\n",
       " 'amours',\n",
       " 'amp',\n",
       " 'ample',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusedly',\n",
       " 'amusement',\n",
       " 'amusements',\n",
       " 'amuses',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'amélie',\n",
       " 'ana',\n",
       " 'anachronistic',\n",
       " 'anakin',\n",
       " 'analgesic',\n",
       " 'analysis',\n",
       " 'analytical',\n",
       " 'analyze',\n",
       " 'anarchic',\n",
       " 'anarchist',\n",
       " 'anarchists',\n",
       " 'anarchy',\n",
       " 'anatomical',\n",
       " 'anchor',\n",
       " 'anchored',\n",
       " 'anchoring',\n",
       " 'anchors',\n",
       " 'ancient',\n",
       " 'anciently',\n",
       " 'ancillary',\n",
       " 'and-miss',\n",
       " 'and\\\\/or',\n",
       " 'anderson',\n",
       " 'andie',\n",
       " 'andrei',\n",
       " 'android',\n",
       " 'andré',\n",
       " 'anecdote',\n",
       " 'anemic',\n",
       " 'anew',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelina',\n",
       " 'angelique',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angles',\n",
       " 'angling',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'angst-ridden',\n",
       " 'anguish',\n",
       " 'anguished',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animated',\n",
       " 'animated-movie',\n",
       " 'animation',\n",
       " 'animations',\n",
       " 'animaton',\n",
       " 'animator',\n",
       " 'animatronic',\n",
       " 'anime',\n",
       " 'animé',\n",
       " 'aniston',\n",
       " 'ankle-deep',\n",
       " 'anna',\n",
       " 'annals',\n",
       " 'anne',\n",
       " 'anne-sophie',\n",
       " 'annex',\n",
       " 'annie',\n",
       " 'annie-mary',\n",
       " 'anniversary',\n",
       " 'annoyance',\n",
       " 'annoyances',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'anomaly',\n",
       " 'anomie',\n",
       " 'anonymity',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'antagonism',\n",
       " 'ante',\n",
       " 'anteing',\n",
       " 'anthology',\n",
       " 'anthony',\n",
       " 'anthropology',\n",
       " 'anthropomorphic',\n",
       " 'anti-',\n",
       " 'anti-adult',\n",
       " 'anti-catholic',\n",
       " 'anti-erotic',\n",
       " 'anti-establishment',\n",
       " 'anti-feminist',\n",
       " 'anti-harry',\n",
       " 'anti-human',\n",
       " 'anti-kieslowski',\n",
       " 'anti-semitism',\n",
       " 'anti-virus',\n",
       " 'anti-war',\n",
       " 'antic',\n",
       " 'anticipated',\n",
       " 'anticipation',\n",
       " 'antics',\n",
       " 'antidote',\n",
       " 'antique',\n",
       " 'antiseptic',\n",
       " 'antitrust',\n",
       " 'anton',\n",
       " 'antonia',\n",
       " 'antonio',\n",
       " 'ants',\n",
       " 'antsy',\n",
       " 'antwone',\n",
       " 'anxieties',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyplace',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apartheid',\n",
       " 'apartments',\n",
       " 'ape',\n",
       " 'apes',\n",
       " 'apex',\n",
       " 'aplenty',\n",
       " 'aplomb',\n",
       " 'apocalypse',\n",
       " 'apollo',\n",
       " 'apology',\n",
       " 'appalling',\n",
       " 'apparatus',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appealingly',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appetite',\n",
       " 'appetites',\n",
       " 'appetizer',\n",
       " 'appetizing',\n",
       " 'applauded',\n",
       " 'apple',\n",
       " 'applegate',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appointed',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'appreciation',\n",
       " 'appreciative',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriated',\n",
       " 'appropriately',\n",
       " 'approximation',\n",
       " 'appétit',\n",
       " 'april',\n",
       " 'apted',\n",
       " 'aptitude',\n",
       " 'aptly',\n",
       " 'aragorn',\n",
       " 'aranda',\n",
       " 'ararat',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arc',\n",
       " 'arcane',\n",
       " 'arch',\n",
       " 'archetypal',\n",
       " 'archibald',\n",
       " 'architect',\n",
       " 'architecture',\n",
       " 'archival',\n",
       " 'archive',\n",
       " 'archives',\n",
       " 'archly',\n",
       " 'arctic',\n",
       " 'ardent',\n",
       " 'ardently',\n",
       " 'ardor',\n",
       " 'arduous',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'argentine',\n",
       " 'argentinean',\n",
       " 'argentinian',\n",
       " 'argento',\n",
       " 'argot',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'argues',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'arise',\n",
       " 'arising',\n",
       " 'aristocracy',\n",
       " 'aristocrat',\n",
       " 'aristocrats',\n",
       " 'arithmetic',\n",
       " 'ark',\n",
       " 'arkansas',\n",
       " 'arliss',\n",
       " 'arm',\n",
       " 'armageddon',\n",
       " 'armchair',\n",
       " 'armed',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'arms',\n",
       " 'arnie',\n",
       " 'arnold',\n",
       " 'around',\n",
       " 'arrangements',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arresting',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'art-conscious',\n",
       " 'art-house',\n",
       " 'artefact',\n",
       " 'arteta',\n",
       " 'artful',\n",
       " 'artfully',\n",
       " 'arthouse',\n",
       " 'arthur',\n",
       " 'articulate',\n",
       " 'articulates',\n",
       " 'artifice',\n",
       " 'artificial',\n",
       " 'artificiality',\n",
       " 'artist',\n",
       " 'artistes',\n",
       " 'artistic',\n",
       " 'artistically',\n",
       " 'artistry',\n",
       " 'artists',\n",
       " 'artless',\n",
       " 'artnering',\n",
       " 'arts',\n",
       " 'artsploitation',\n",
       " 'artsy',\n",
       " 'artwork',\n",
       " 'artworks',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_ig = IntegratedGradients(sst_classifier.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All-0s baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_baseline = torch.zeros(1, experiment['train_dataset']['X'].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributions with respect to the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_attrs = sst_ig.attribute(\n",
    "    torch.FloatTensor(X_sst_test),\n",
    "    sst_baseline,\n",
    "    target=torch.LongTensor(sst_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for error analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(gold=1, predicted=2):\n",
    "    err_ind = [i for i, (g, p) in enumerate(zip(y_sst_test, sst_preds))\n",
    "               if g == gold and p == predicted]\n",
    "    attr_lookup = create_attr_lookup(sst_attrs[err_ind])\n",
    "    return attr_lookup, err_ind\n",
    "\n",
    "def create_attr_lookup(attrs):\n",
    "    mu = attrs.mean(axis=0).detach().numpy()\n",
    "    return sorted(zip(fnames, mu), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_attrs_lookup, sst_err_ind = error_analysis(gold=1, predicted=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.11495877629674077),\n",
       " ('fun', 0.06644722713237726),\n",
       " ('film', 0.04247883257980162),\n",
       " ('solid', 0.04107643978858098),\n",
       " ('makes', 0.03878941709677621)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_attrs_lookup[: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis for a specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_ind = sst_err_ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No one goes unindicted here , which is probably for the best .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment['assess_datasets'][0]['raw_examples'][ex_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_attr_lookup = create_attr_lookup(sst_attrs[ex_ind:ex_ind+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('best', 0.9777032930161551),\n",
       " ('probably', 0.23590500013472715),\n",
       " ('.', 0.11888355179360353),\n",
       " (',', 0.03269009976933634),\n",
       " ('one', 0.00562289517486443),\n",
       " ('goes', -0.06158406715880671)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(f, a) for f, a in ex_attr_lookup if a != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_weights_name = 'cardiffnlp/twitter-roberta-base-sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = AutoModelForSequenceClassification.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_predict_one_proba(text):\n",
    "    input_ids = hf_tokenizer.encode(\n",
    "        text, add_special_tokens=True, return_tensors='pt')\n",
    "    hf_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = hf_model(input_ids)[0]\n",
    "        preds = F.softmax(logits, dim=1)\n",
    "    hf_model.train()\n",
    "    return preds.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_ig_encodings(text):\n",
    "    pad_id = hf_tokenizer.pad_token_id\n",
    "    cls_id = hf_tokenizer.cls_token_id\n",
    "    sep_id = hf_tokenizer.sep_token_id\n",
    "    input_ids = hf_tokenizer.encode(text, add_special_tokens=False)\n",
    "    base_ids = [pad_id] * len(input_ids)\n",
    "    input_ids = [cls_id] + input_ids + [sep_id]\n",
    "    base_ids = [cls_id] + base_ids + [sep_id]\n",
    "    return torch.LongTensor([input_ids]), torch.LongTensor([base_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_ig_analyses(text2class):\n",
    "    data = []\n",
    "    for text, true_class in text2class.items():\n",
    "        score_vis = hf_ig_analysis_one(text, true_class)\n",
    "        data.append(score_vis)\n",
    "    viz.visualize_text(data)\n",
    "\n",
    "\n",
    "def hf_ig_analysis_one(text, true_class):\n",
    "    # Option to look at different layers:\n",
    "    # layer = model.roberta.encoder.layer[0]\n",
    "    # layer = model.roberta.embeddings.word_embeddings\n",
    "    layer = hf_model.roberta.embeddings\n",
    "\n",
    "    def ig_forward(inputs):\n",
    "        return hf_model(inputs).logits\n",
    "\n",
    "    ig = LayerIntegratedGradients(ig_forward, layer)\n",
    "\n",
    "    input_ids, base_ids = hf_ig_encodings(text)\n",
    "\n",
    "    attrs, delta = ig.attribute(\n",
    "        input_ids,\n",
    "        base_ids,\n",
    "        target=true_class,\n",
    "        return_convergence_delta=True)\n",
    "\n",
    "    # Summarize and z-score normalize the attributions\n",
    "    # for each representation in `layer`:\n",
    "    scores = attrs.sum(dim=-1).squeeze(0)\n",
    "    scores = (scores - scores.mean()) / scores.norm()\n",
    "\n",
    "    # Intuitive tokens to help with analysis:\n",
    "    raw_input = hf_tokenizer.convert_ids_to_tokens(input_ids.tolist()[0])\n",
    "    # RoBERTa-specific clean-up:\n",
    "    raw_input = [x.strip(\"Ġ\") for x in raw_input]\n",
    "\n",
    "    # Predictions for comparisons:\n",
    "    pred_probs = hf_predict_one_proba(text)\n",
    "    pred_class = pred_probs.argmax()\n",
    "\n",
    "    score_vis = viz.VisualizationDataRecord(\n",
    "        word_attributions=scores,\n",
    "        pred_prob=pred_probs.max(),\n",
    "        pred_class=pred_class,\n",
    "        true_class=true_class,\n",
    "        attr_class=None,\n",
    "        attr_score=attrs.sum(),\n",
    "        raw_input_ids=raw_input,\n",
    "        convergence_score=delta)\n",
    "\n",
    "    return score_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.82)</b></text></td><td><text style=\"padding-right:2em\"><b>None</b></text></td><td><text style=\"padding-right:2em\"><b>1.98</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> They                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> right                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.50)</b></text></td><td><text style=\"padding-right:2em\"><b>None</b></text></td><td><text style=\"padding-right:2em\"><b>0.07</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> They                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> wrong                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.76)</b></text></td><td><text style=\"padding-right:2em\"><b>None</b></text></td><td><text style=\"padding-right:2em\"><b>2.39</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> They                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> right                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> say                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.62)</b></text></td><td><text style=\"padding-right:2em\"><b>None</b></text></td><td><text style=\"padding-right:2em\"><b>3.46</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> They                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> wrong                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> say                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2</b></text></td><td><text style=\"padding-right:2em\"><b>2 (0.77)</b></text></td><td><text style=\"padding-right:2em\"><b>None</b></text></td><td><text style=\"padding-right:2em\"><b>1.78</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> They                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stellar                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> correct                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.47)</b></text></td><td><text style=\"padding-right:2em\"><b>None</b></text></td><td><text style=\"padding-right:2em\"><b>1.17</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> They                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stellar                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 58%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> incorrect                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_vis = hf_ig_analyses({\n",
    "    \"They said it would be great, and they were right.\": 2,\n",
    "    \"They said it would be great, and they were wrong.\": 0,\n",
    "    \"They were right to say it would be great.\": 2,\n",
    "    \"They were wrong to say it would be great.\": 0,\n",
    "    \"They said it would be stellar, and they were correct.\": 2,\n",
    "    \"They said it would be stellar, and they were incorrect.\": 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
